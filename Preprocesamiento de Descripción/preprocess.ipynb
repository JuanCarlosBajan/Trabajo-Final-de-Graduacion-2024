{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI MODEL DESCRIPTION PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from rapidfuzz import process, fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al directorio que contiene los archivos JSON\n",
    "json_directory_path = '../../Invoice_Downloader'\n",
    "\n",
    "# Inicializar lista para almacenar los datos\n",
    "data_list = []\n",
    "\n",
    "# Iterar sobre todos los archivos en el directorio\n",
    "for filename in os.listdir(json_directory_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(json_directory_path, filename)\n",
    "        \n",
    "        # Cargar los datos del archivo JSON\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Filtrar y extraer descripciones de productos\n",
    "        for item in data:\n",
    "            if item.get('accounting_classification', '') != '' and item.get('initial_description', '') != '':\n",
    "                data_list.append({\n",
    "                    'initial_description': item['initial_description'],\n",
    "                    'final_description': item['final_description'],\n",
    "                    'classification': item['accounting_classification'],\n",
    "                    'unit_total': item['unit_total'],\n",
    "                    'company_tid': item['company_tid'],\n",
    "                    'establishment_id':item['establishment_id']\n",
    "                })\n",
    "\n",
    "# Crear un DataFrame con los datos recopilados\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING RESOURCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\juanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inicializar lematizador y lista de stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Cargar listado de palabras en español (listado corto)\n",
    "with open('spanish_words.txt', 'r', encoding='utf-8') as file:\n",
    "    spanish_words = set(file.read().splitlines())\n",
    "\n",
    "# Lista para almacenar palabras no válidas\n",
    "invalid_words = []\n",
    "\n",
    "global_word_frequencies = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para encontrar la palabra más cercana en el diccionario usando rapidfuzz\n",
    "def get_closest_word(word, dictionary):\n",
    "    closest_match = process.extractOne(word, dictionary, scorer=fuzz.ratio, score_cutoff=80)  # Usar umbral del 80% de similitud\n",
    "    if closest_match:\n",
    "        return closest_match[0]\n",
    "    else:\n",
    "        invalid_words.append(word)\n",
    "        return ''\n",
    "    \n",
    "\n",
    "def save_frequencies_to_file(filename, frequencies):\n",
    "    # Guarda las frecuencias en un archivo de texto legible\n",
    "    with open(filename, 'w') as file:\n",
    "        for word, frequency in frequencies.items():\n",
    "            file.write(f\"{word}: {frequency}\\n\")\n",
    "\n",
    "def save_invalid_words_to_file(filename, invalid_words):\n",
    "    # Guarda las palabras inválidas en un archivo de texto legible\n",
    "    with open(filename, 'w') as file:\n",
    "        for word in invalid_words:\n",
    "            file.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1\n",
    "# Clean unexpected characters and lower description\n",
    "def step_1(description):\n",
    "    # Convertir a minúsculas\n",
    "    description = description.lower()\n",
    "    # Reemplazar caracteres especiales con espacios, excepto el signo de interrogación\n",
    "    description = re.sub(r'[^\\w\\s\\?]', ' ', description)\n",
    "    # Eliminar signos de interrogación\n",
    "    description = description.replace('?', '')\n",
    "    # Eliminar dígitos\n",
    "    description = re.sub(r'\\d+', '', description)\n",
    "    # Eliminar espacios extra\n",
    "    description = re.sub(r'\\s+', ' ', description).strip()\n",
    "    # Eliminar stop words\n",
    "    #words = description.split()\n",
    "    #filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ''.join(description)\n",
    "\n",
    "# Paso 2\n",
    "# Tokenize and filter small words\n",
    "def step_2(description):\n",
    "    # Tokenizar la descripción en palabras\n",
    "    words = description.split()\n",
    "    # Filtrar palabras con tres o menos caracteres\n",
    "    filtered_words = [word for word in words if len(word) > 3]\n",
    "    return filtered_words\n",
    "\n",
    "# Paso 3\n",
    "# Ortografic Changes\n",
    "def step_3(words):\n",
    "    # Corrige las palabras usando la función get_closest_word\n",
    "    corrected_words = [get_closest_word(word, spanish_words) for word in words]\n",
    "    \n",
    "    return corrected_words\n",
    "\n",
    "def step_4(words):\n",
    "    # Actualiza el diccionario global con la frecuencia de cada palabra\n",
    "    global_word_frequencies.update(words)\n",
    "    return dict(global_word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_description(description):\n",
    "    description = step_1(description)\n",
    "\n",
    "    #words = step_2(description)\n",
    "\n",
    "    #or_changes = step_3(words)\n",
    "\n",
    "    #step_4(or_changes)\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3458, 7)\n"
     ]
    }
   ],
   "source": [
    "# Aplica la función a la columna 'description'\n",
    "df['cleaned_initial_description'] = df['initial_description'].apply(process_description)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado en el archivo CSV: cleaned_descriptions.csv\n"
     ]
    }
   ],
   "source": [
    "# Guarda las frecuencias en un archivo de texto\n",
    "#save_frequencies_to_file('word_frequencies.txt', global_word_frequencies)\n",
    "#save_invalid_words_to_file('invalid_words.txt', invalid_words)\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV con codificación UTF-8-SIG\n",
    "csv_file_path = 'cleaned_descriptions.csv'\n",
    "df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"DataFrame guardado en el archivo CSV: {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
